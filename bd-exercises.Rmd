---
title: "bd-exercises"
author: "Alejandro Jim√©nez Rico"
date: "5 April 2018"
output: html_document
---

# ISLR EXERCISES

### EXERCISE 3

**a)**

IV is correct. Since $s$ is increasing, the model allows more variables to fit the regression, thus letting the training RSS to decrease.

**b)**

II is correct. Adding more variables to the regression can reduce the test RSS, since the fit is being improved with more information. When too many variables are included in the regression, the model starts overfitting due to an excess of information, being trained with too much noise and thus increasing the test RSS.

**c)**

III is correct. Less constraints allows to more variables, thus increasing the variance.

**d)**

IV is correct. High bias is what causes an algorithm to _miss_ some relevant information, thus underfitting. Logically, as we inccrease the number of variables we should reduce the squared bias.

**e)**

V is correct. Irreductible error does not depend on the complexity (or number of variables) of the model.

### EXERCISE 5


**a)**
In a Ridge Regression, we would need to minimize

$(y_1 - \hat(\beta)_1 x_{11} - \hat{\beta}_2 x_{12})^2 + (y_2 - \hat{\beta}_1 x_{21} - \hat{\beta}_2 x_{22})^2 + \lambda (\hat{\beta}_1^2 + \hat{\beta}_2^2)$

**b)**

If we expand the previous equation, we may find 
$$(y_1 - \hat\beta_1 x_{11} - \hat\beta_2 x_{12})^2 + (y_2 - \hat\beta_1 x_{21} - \hat\beta_2 x_{22})^2 + \lambda (\hat\beta_1^2 + \hat\beta_2^2) = (y_1^2 + \hat\beta_1^2 x_{11}^2 + \hat\beta_2^2 x_{12}^2 - 2 \hat\beta_1 x_{11} y_1 - 2 \hat\beta_2 x_{12} y_1 + 2 \hat\beta_1 \hat\beta_2 x_{11} x_{12}).$$  

And thus, 

$$y_2^2 + \hat\beta_1^2 x_{21}^2 + \hat\beta_2^2 x_{22}^2 - 2 \hat\beta_1 x_{21} y_2 - 2 \hat\beta_2 x_{22} y_2 + 2 \hat\beta_1 \hat\beta_2 x_{21} x_{22})
\lambda \hat\beta_1^2 + \lambda \hat\beta_2^2.$$ 

Next, we take the partial dervative to $\hat\beta_1$ and set the equation to 0, in order to minimze 

$$\frac{\partial }{\partial \hat\beta_1}: (2\hat\beta_1x_{11}^2-2x_{11}y_1+2\hat\beta_2x_{11}x_{12}) + (2\hat\beta_1x_{21}^2-2x_{21}y_2+2\hat\beta_2x_{21}x_{22}) + 2\lambda\hat\beta_1 = 0.$$  

Now, if we set $x_{11}=x_{12}=x_1$ and $x_{21}=x_{22}=x_2$, and we divide both sides of the equation by $2$, we my find 

$$(\hat\beta_1x_1^2-x_1y_1+\hat\beta_2x_1^2) + (\hat\beta_1x_2^2-x_2y_2+\hat\beta_2x_2^2) + \lambda\hat\beta_1 = 0;$$ 

which is

$$\hat\beta_1 (x_1^2+x_2^2) + \hat\beta_2 (x_1^2+x_2^2) + \lambda\hat\beta_1 = x_1y_1 + x_2y_2.$$

At this moment, we may add $2\hat\beta_1x_1x_2$ and $2\hat\beta_2x_1x_2$ to both sides of the equation, and consequently find that

$$\hat\beta_1 (x_1^2 + x_2^2 + 2x_1x_2) + \hat\beta_2 (x_1^2 + x_2^2 + 2x_1x_2) + \lambda\hat\beta_1 = x_1y_1 + x_2y_2 + 2\hat\beta_1x_1x_2 + 2\hat\beta_2x_1x_2 \hat\beta_1 (x_1 + x_2)^2 + \hat\beta_2 (x_1 + x_2)^2 + \lambda\hat\beta_1 $$ 

$$= x_1y_1 + x_2y_2 + 2\hat\beta_1x_1x_2 + 2\hat\beta_2x_1x_2.$$


And because $x_1 + x_2 = 0$, we can remove the forst two terms and find that

$$\lambda\hat\beta_1 = x_1y_1 + x_2y_2 + 2\hat\beta_1x_1x_2 + 2\hat\beta_2x_1x_2.$$

Analogously, by taking the partial derivative to $2\hat\beta_2$, we obtain

$$\lambda\hat\beta_2 = x_1y_1 + x_2y_2 + 2\hat\beta_1x_1x_2 + 2\hat\beta_2x_1x_2.$$

At this point, we should notice that the left side of the equations for both $\lambda\hat\beta_1$ and $\lambda\hat\beta_2$ are the same. Hence we have 

$$\lambda\hat\beta_1 = \lambda\hat\beta_2,$$
and thus

$$\hat\beta_1 = \hat\beta_2.$$
**c)**

For a _Lasso_ regression, we want to minimize $(y_1 - \hat\beta_1x_{11} - \hat\beta_2x_{12})^2 + (y_2 - \hat\beta_1x_{21} - \hat\beta_2x_{22})^2 + \lambda (|\hat\beta_1| + |\hat\beta_2|).$

**d)**

Substituting the constraint term from part *b)*, the derivative term to $\beta$ is:

$$\frac{\partial }{\partial \hat\beta} (\lambda |\beta|): \lambda\frac{|\beta|}{\beta}$$

Following through the steps in part *b)*, we obtain

$$\lambda\frac{|\beta_1|}{\beta_1} = \lambda\frac{|\beta_2|}{\beta_2}.$$

Therefore, we can infere the Lasso just requires that $\beta_1$ and $\beta_2$ are both positive or both negative.


### EXERCISE 8


**a)**
```{r}
set.seed(666)
x <- rnorm(100)
epsilon <- rnorm(100)
```
**b)**

```{r}
beta0 <- 42
set.seed(666)
beta <- trunc(runif(7)*10)

y <- beta0 + beta[[1]]*x - beta[[2]]*x^2 + beta[[3]]*x^3 + epsilon
```


**c)**
```{r} 
library(tidyverse)
library(leaps)
library(reshape2)

regfit.full <- regsubsets(y~poly(x,10,raw=T), 
													data=data.frame(y,x), nvmax=10)

(reg.summary <- summary(regfit.full))
min.cp <- which.min(reg.summary$cp)  
min.bic <- which.min(reg.summary$bic)  
min.adjr2 <- which.max(reg.summary$adjr2)  

df <- as_tibble(data.frame(cp = reg.summary$cp, bic = reg.summary$bic, adjr2 = reg.summary$adjr2))
df$row <- as.factor(rownames(df))
df <- df %>% melt()

df2 <- df %>% 
	group_by(variable) %>%
	summarise(min = min(value),
						max = max(value))

df3 <- left_join(df, df2) %>% 
	mutate(color1 = (value == min & variable %in% c("cp","bic"))) %>% 
	mutate(color2 = (value == max & variable %in% c("adjr2"))) %>% 
	filter(color1 == TRUE | color2 == TRUE)

df %>% 
	ggplot() +
	geom_line(aes(y = value, x = as.integer(row), colour = variable), size =1.3) +
	geom_point(data = df3, aes(x = as.integer(row), y = value), color = "black", size = 3) +
	xlab("Number of Poly(x)") +
	ylab("Best Subset of") +
	facet_wrap(~variable, nrow=3, scales = "free") +
	labs(colour = "") +
	theme_minimal() +
	scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10))

coef(regfit.full, min.cp)
coef(regfit.full, min.bic)
coef(regfit.full, min.adjr2)
rm(df)
```

**d)**

```{r}
# forward selection
regfit.fwd <- regsubsets(y~poly(x,10,raw=T), data=data.frame(y,x), nvmax=10)
(fwd.summary <- summary(regfit.fwd))

# backward selection
regfit.bwd <- regsubsets(y~poly(x,10,raw=T), data=data.frame(y,x), nvmax=10)
(bwd.summary <- summary(regfit.bwd))

par(mfrow=c(3,2))

min.cp <- which.min(fwd.summary$cp)  
min.cp <- which.min(bwd.summary$cp)  
min.bic <- which.min(fwd.summary$bic)
min.bic <- which.min(bwd.summary$bic)
min.adjr2 <- which.max(fwd.summary$adjr2)
min.adjr2 <- which.max(bwd.summary$adjr2) 

# Forward
fwd.df <- data.frame(cp = fwd.summary$cp, bic = fwd.summary$bic, adjr2 = fwd.summary$adjr2)
fwd.df$row <- as.factor(rownames(fwd.df))
fwd.df <- fwd.df %>% melt()

fwd.df2 <- fwd.df %>% 
	group_by(variable) %>%
	summarise(min = min(value),
						max = max(value))

fwd.df3 <- left_join(fwd.df, fwd.df2) %>% 
	mutate(color1 = (value == min & variable %in% c("cp","bic"))) %>% 
	mutate(color2 = (value == max & variable %in% c("adjr2"))) %>% 
	filter(color1 == TRUE | color2 == TRUE)

fwd.df %>% 
	ggplot() +
	geom_line(aes(y = value, x = as.integer(row), colour = variable), size =1.3) +
	geom_point(data = fwd.df3, aes(x = as.integer(row), y = value), color = "black", size = 3) +
	xlab("Number of Poly(x)") +
	ylab("Forward Selection of ") +
	facet_wrap(~variable, nrow=3, scales = "free") +
	labs(colour = "") +
	theme_minimal() +
	scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10))

# Backward
bwd.df <- data.frame(cp = bwd.summary$cp, bic = bwd.summary$bic, adjr2 = bwd.summary$adjr2)
bwd.df$row <- as.factor(rownames(bwd.df))
bwd.df <- bwd.df %>% melt()

bwd.df2 <- bwd.df %>% 
	group_by(variable) %>%
	summarise(min = min(value),
						max = max(value))

bwd.df3 <- left_join(bwd.df, bwd.df2) %>% 
	mutate(color1 = (value == min & variable %in% c("cp","bic"))) %>% 
	mutate(color2 = (value == max & variable %in% c("adjr2"))) %>% 
	filter(color1 == TRUE | color2 == TRUE)

bwd.df %>% 
	ggplot() +
	geom_line(aes(y = value, x = as.integer(row), colour = variable), size =1.3) +
	geom_point(data = bwd.df3, aes(x = as.integer(row), y = value), color = "black", size = 3) +
	xlab("Number of Poly(x)") +
	ylab("Backward Selection of ") +
	facet_wrap(~variable, nrow=3, scales = "free") +
	labs(colour = "") +
	theme_minimal() +
	scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10))






# coefficients of selected models
coef(regfit.fwd, which.min(fwd.summary$cp))
coef(regfit.bwd, which.min(bwd.summary$cp))

coef(regfit.fwd, which.min(fwd.summary$bic))
coef(regfit.bwd, which.min(bwd.summary$bic))

coef(regfit.fwd, which.max(fwd.summary$adjr2))
coef(regfit.bwd, which.max(bwd.summary$adjr2))
```

**e)**

```{r}
library(glmnet)
xmat <- model.matrix(y~poly(x,10,raw=T))[,-1]
lasso.mod <- cv.glmnet(xmat, y, alpha=1)
(lambda <- lasso.mod$lambda.min)
par(mfrow=c(1,1))
plot(lasso.mod)
predict(lasso.mod, s=lambda, type="coefficients")
```

Now if we remember our $\beta_0$ and $\beta_i$, we may conclude that the best results come from the _cp_ model and _bic_.

```{r}
print(beta0)
print(beta)
```


**f)**

```{r}
y2 <- beta0 + beta[[7]]*x^7 + epsilon

# best subset model selection
regfit.full <- regsubsets(y2~poly(x,10,raw=T), data=data.frame(y,x), nvmax=10)
(reg.summary <- summary(regfit.full))
min.cp <- which.min(reg.summary$cp)  
min.bic <- which.min(reg.summary$bic)  
min.adjr2 <- which.max(reg.summary$adjr2)  
coef(regfit.full, min.cp)
coef(regfit.full, min.bic)
coef(regfit.full, min.adjr2)

# Reg
reg.df <- data.frame(cp = reg.summary$cp, bic = reg.summary$bic, adjr2 = reg.summary$adjr2)
reg.df$row <- as.factor(rownames(reg.df))
reg.df <- reg.df %>% melt()

reg.df2 <- reg.df %>% 
	group_by(variable) %>%
	summarise(min = min(value),
						max = max(value))

reg.df3 <- left_join(reg.df, reg.df2) %>% 
	mutate(color1 = (value == min & variable %in% c("cp","bic"))) %>% 
	mutate(color2 = (value == max & variable %in% c("adjr2"))) %>% 
	filter(color1 == TRUE | color2 == TRUE)

reg.df %>% 
	ggplot() +
	geom_line(aes(y = value, x = as.integer(row), colour = variable), size =1.3) +
	geom_point(data = reg.df3, aes(x = as.integer(row), y = value), color = "black", size = 3) +
	xlab("Number of Poly(x)") +
	ylab("Backward Selection of ") +
	facet_wrap(~variable, nrow=3, scales = "free") +
	labs(colour = "") +
	theme_minimal() +
	scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10))



# lasso regression 
xmat <- model.matrix(y2~poly(x,10,raw=T))[,-1]
lasso.mod <- cv.glmnet(xmat, y2, alpha=1)
(lambda <- lasso.mod$lambda.min)
par(mfrow=c(1,1))
plot(lasso.mod)
predict(lasso.mod, s=lambda, type="coefficients")
```

This time, the most accurate results are shown by the _adjr2_ model.


### EXERCISE 9

**a)**

```{r}
library(ISLR)
data(College)
set.seed(1)
trainid <- sample(1:nrow(College), nrow(College)/2)
train <- College[trainid,]
test <- College[-trainid,]
str(College)
```

**b)**

```{r}
fit.lm <- lm(Apps~., data=train)
pred.lm <- predict(fit.lm, test)
(err.lm <- mean((test$Apps - pred.lm)^2))  # test error
```

**c)**

```{r}
library(glmnet)
xmat.train <- model.matrix(Apps~., data=train)[,-1]
xmat.test <- model.matrix(Apps~., data=test)[,-1]
fit.ridge <- cv.glmnet(xmat.train, train$Apps, alpha=0)
(lambda <- fit.ridge$lambda.min)  # optimal lambda
pred.ridge <- predict(fit.ridge, s=lambda, newx=xmat.test)
(err.ridge <- mean((test$Apps - pred.ridge)^2))  # test error
```

**d)**

```{r}
library(glmnet)
xmat.train <- model.matrix(Apps~., data=train)[,-1]
xmat.test <- model.matrix(Apps~., data=test)[,-1]
fit.lasso <- cv.glmnet(xmat.train, train$Apps, alpha=1)
(lambda <- fit.lasso$lambda.min)  # optimal lambda
pred.lasso <- predict(fit.lasso, s=lambda, newx=xmat.test)
(err.lasso <- mean((test$Apps - pred.lasso)^2))  # test error
coef.lasso <- predict(fit.lasso, type="coefficients", s=lambda)[1:ncol(College),]
coef.lasso[coef.lasso != 0]
length(coef.lasso[coef.lasso != 0])
```

**g)**

```{r}
library(viridis)
err.all <- c(err.lm, err.ridge, err.lasso)
df <- as.data.frame(err.all)
df$names<- c("lm", "ridge", "lasso")
 df%>% 
	ggplot() +
	geom_bar(aes(y = err.all, x = names, fill = names), stat = "identity") +
 	scale_fill_viridis(discrete = TRUE) +
 	theme_minimal() +
 	theme(legend.position="none") + 
 	ylab("Value")
 
 
```

### EXERCISE 10

**a)**

```{r}
set.seed(666)
epsilon <- rnorm(1000)
xmat <- matrix(rnorm(1000*20), ncol=20)
beta <- round(runif(20)*10-5)
y <- xmat %*% beta + epsilon
```

**b)**

```{r}
set.seed(666)
trainid <- sample(1:1000, 100, replace = FALSE)
xmat.train <- xmat[trainid,]
xmat.test <- xmat[-trainid,]
y.train <- y[trainid,]
y.test <- y[-trainid,]
train <- data.frame(y=y.train, xmat.train)
test <- data.frame(y=y.test, xmat.test)
```

**c)**

```{r}
library(leaps)

predict.regsubsets <- function(object, newdata, id, ...){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id=id)
  xvars <- names(coefi)
  mat[,xvars]%*%coefi
}

regfit.full <- regsubsets(y~., data=train, nvmax=20)
err.full <- rep(NA, 20)
for(i in 1:20) {
  pred.full <- predict(regfit.full, train, id=i)
  err.full[i] <- mean((train$y - pred.full)^2)
}
plot(1:20, err.full, type="b", main="Training MSE", xlab="Number of Predictors")
which.min(err.full)  # min for train error should be at max pred count
```

**d)**

```{r}
err.full <- rep(NA, 20)
for(i in 1:20) {
  pred.full <- predict(regfit.full, test, id=i)
  err.full[i] <- mean((test$y - pred.full)^2)
}
plot(1:20, err.full, type="b", main="Test MSE", xlab="Number of Predictors")
```

**e)**

```{r}
which.min(err.full)
```

**f)**

```{r}
(coef.best <- coef(regfit.full, id=which.min(err.full)))
beta[beta != 0]
names(beta) <- paste0("X", 1:20)
merge(data.frame(beta=names(beta),beta), data.frame(beta=names(coef.best),coef.best), all.x=T, sort=F)
```

**g)**

```{r}
err.coef <- rep(NA, 20)
for(i in 1:20) {
  coef.i <- coef(regfit.full, id=i)
  df.err <- merge(data.frame(beta=names(beta),beta), data.frame(beta=names(coef.i),coef.i), all.x=T)
  df.err[is.na(df.err[,3]),3] <- 0
  err.coef[i] <- sqrt(sum((df.err[,2] - df.err[,3])^2))
}
plot(1:20, err.coef, type="b", main="Coefficient Error", xlab="Number of Predictors")
points(which.min(err.coef), err.coef[which.min(err.coef)], col="red", pch=16)
```





